# Ollamaè‡ªå®šä¹‰å¯¼å…¥æ¨¡å‹

## ç®€ä»‹
æœ¬èŠ‚å­¦ä¹ å¦‚ä½•ä½¿ç”¨Modelfileæ¥è‡ªå®šä¹‰å¯¼å…¥æ¨¡å‹ï¼Œä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†:
1. ä»GGUFå¯¼å…¥
2. ä»Pytorch æˆ– Safetensorså¯¼å…¥
3. ç”±æ¨¡å‹ç›´æ¥å¯¼å…¥
4. è‡ªå®šä¹‰Prompt


## ä¸€ã€ä»GGUFå¯¼å…¥

GGUF (GPT-Generated Unified Format) æ˜¯ä¸€ç§æ–‡ä»¶æ ¼å¼ï¼Œç”¨äºä¿å­˜ç»è¿‡å¾®è°ƒçš„è¯­è¨€æ¨¡å‹ã€‚è¿™ç§æ ¼å¼æ—¨åœ¨å¸®åŠ©ç”¨æˆ·æ–¹ä¾¿åœ°åœ¨ä¸åŒçš„å¹³å°å’Œç¯å¢ƒä¹‹é—´å…±äº«å’Œå¯¼å…¥æ¨¡å‹ã€‚å®ƒæ”¯æŒå¤šç§é‡åŒ–æ ¼å¼ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘æ¨¡å‹æ–‡ä»¶çš„å¤§å°ã€‚


å®ƒçš„å‰èº«æ˜¯GGML(GPT-Generated Model Language)ï¼Œæ˜¯ä¸“é—¨ä¸ºäº†æœºå™¨å­¦ä¹ è€Œè®¾è®¡çš„Tensoråº“ï¼Œç›®çš„æ˜¯ä¸ºäº†æœ‰ä¸€ä¸ªå•æ–‡ä»¶çš„æ ¼å¼ï¼Œå¹¶ä¸”æ˜“åœ¨ä¸åŒæ¶æ„çš„CPUä»¥åŠGPUä¸Šå¯ä»¥æ¨ç†ï¼Œä½†åç»­ç”±äºå¼€å‘é‡åˆ°äº†çµæ´»æ€§ä¸è¶³ã€ç›¸å®¹æ€§åŠéš¾ä»¥ç»´æŠ¤çš„é—®é¢˜ã€‚


Ollama æ”¯æŒä» GGUF æ–‡ä»¶å¯¼å…¥æ¨¡å‹ï¼Œé€šè¿‡ä»¥ä¸‹æ­¥éª¤æ¥å®ç°ï¼š
1. ä¸‹è½½.gguf æ–‡ä»¶

    > ä¸‹è½½é“¾æ¥ï¼šhttps://huggingface.co/RichardErkhov/Qwen_-_Qwen2-0.5B-gguf/resolve/main/Qwen2-0.5B.Q3_K_M.gguf?download=true

    ä¸ºäº†æ¼”ç¤ºçš„æ–¹ä¾¿ï¼Œæˆ‘ä»¬é€‰ç”¨äº† Qwen2-0.5B æ¨¡å‹ã€‚ä¸‹è½½åå¤åˆ¶åˆ°ç¬¬ä¸€éƒ¨åˆ†çš„æ ¹ç›®å½•ä¸‹

2. æ–°å»ºåˆ›å»ºModelfileæ–‡ä»¶
    ```python
    FROM ./Qwen2-0.5B.Q3_K_M.gguf
    ```

3. åœ¨Ollamaä¸­åˆ›å»ºæ¨¡å‹
    ```bash
    ollama create mymodel -f Modelfile
    ```

4. ç»ˆç«¯å†…è¿è¡Œæ¨¡å‹ï¼ˆå®æµ‹åœ¨notebookä¸­è¿è¡Œä¸å‡ºæ¥ï¼Œæœ€å¥½åœ¨ç»ˆç«¯è¿è¡Œï¼‰
    ```bash
    ollama run mymodel
    ```
![alt text](../images/C3-1-2.png)


## äºŒã€ä»Pytorch æˆ– Safetensorså¯¼å…¥

Safetensors æ˜¯ä¸€ç§ç”¨äºå­˜å‚¨æ·±åº¦å­¦ä¹ æ¨¡å‹æƒé‡çš„æ–‡ä»¶æ ¼å¼ï¼Œå®ƒæ—¨åœ¨è§£å†³å®‰å…¨æ€§ã€æ•ˆç‡å’Œæ˜“ç”¨æ€§æ–¹é¢çš„é—®é¢˜ã€‚ç›®å‰è¿™éƒ¨åˆ†åŠŸèƒ½è¿˜æœ‰å¾…ç¤¾åŒºæˆå‘˜å¼€å‘ï¼Œç›®å‰æ–‡æ¡£èµ„æºæœ‰é™ã€‚

å¦‚æœæ­£åœ¨å¯¼å…¥çš„æ¨¡å‹æ˜¯ä»¥ä¸‹æ¶æ„ä¹‹ä¸€ï¼Œåˆ™å¯ä»¥é€šè¿‡Modelfileç›´æ¥å¯¼å…¥Ollamaã€‚å½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥å°†safetensorsæ–‡ä»¶è½¬æ¢ä¸ºggufæ–‡ä»¶ï¼Œå†è¿›è¡Œå¤„ç†ï¼Œè½¬æ¢è¿‡ç¨‹å¯ä»¥å‚è€ƒç¬¬ä¸‰éƒ¨åˆ†ã€‚

> æœ‰å…³safetensorsä»¥åŠGGUFæ›´è¯¦ç»†çš„ä¿¡æ¯å¯ä»¥å‚è€ƒè¿™ä¸ªé“¾æ¥è¿›è¡Œå­¦ä¹  https://www.linkedin.com/pulse/llama-3-safetensors-vs-gguf-talles-carvalho-jjcqf

- LlamaForCausalLM
- MistralForCausalLM
- GemmaForCausalLM

ç”±äºè¿™éƒ¨åˆ†å†…å®¹ç¤¾åŒºè¿˜åœ¨ä¸æ–­ä¼˜åŒ–ä¸­ï¼Œå› æ­¤ï¼Œè¿™é‡Œæä¾›çš„ç¤ºä¾‹ä»£ç å’Œæµç¨‹ä»…ä¾›å‚è€ƒï¼Œå¹¶ä¸ä¿è¯èƒ½æˆåŠŸè¿è¡Œã€‚è¯¦æƒ…è¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£ã€‚

1. ä¸‹è½½llama-3æ¨¡å‹

    ```python
    !pip install huggingface_hub
    ```

    ```python
    # ä¸‹è½½æ¨¡å‹
    from huggingface_hub import snapshot_download

    model_id = "unsloth/llama-3-8b-bnb-4bit"
    snapshot_download(
        repo_id=model_id, 
        local_dir="llama-3-8b-bnb-4bit",
        local_dir_use_symlinks=False,
        revision="main",
        # æ€ä¹ˆè·å–<YOUR_ACCESS_TOKEN>ï¼Œè¯·å‚ç…§éƒ¨åˆ†3
        use_auth_token="<YOUR_ACCESS_TOKEN>")
    ```

2. æ ¹ç›®å½•ä¸‹åˆ›å»ºModelfileæ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹ï¼š
    ```python
    FROM ./llama-3-8b-bnb-4bit
    ```
3. åœ¨Ollamaä¸­åˆ›å»ºæ¨¡å‹
    ```bash
    ollama create mymodel2 -f Modelfile
    ```
4. è¿è¡Œæ¨¡å‹
    ```bash
    ollama run mymodel2
    ```

## ä¸‰ã€ç”±æ¨¡å‹ç›´æ¥å¯¼å…¥

æ­£å¸¸æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨HuggingFaceæ¥è§¦åˆ°çš„æ¨¡å‹æ–‡ä»¶éå¸¸ä¹‹å¤šï¼Œåº†å¹¸çš„æ˜¯ï¼Œhfæä¾›äº†éå¸¸ä¾¿æ·çš„APIæ¥ä¸‹è½½å’Œå¤„ç†è¿™äº›æ¨¡å‹ï¼Œåƒä¸Šé¢é‚£æ ·ç›´æ¥ä¸‹è½½å—é™äºç½‘ç»œç¯å¢ƒï¼Œé€Ÿåº¦éå¸¸æ…¢ï¼Œè¿™ä¸€å°æ®µæˆ‘ä»¬æ¥ä½¿ç”¨è„šæœ¬ä»¥åŠhfæ¥å®Œæˆã€‚

llama.cppæ˜¯GGUFçš„å¼€æºé¡¹ç›®ï¼Œæä¾›CLIå’ŒServeråŠŸèƒ½

å¯¹äºä¸èƒ½é€šè¿‡Ollamaç›´æ¥è½¬æ¢çš„æ¶æ„ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨llama.cppè¿›è¡Œé‡åŒ–ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºGGUFæ ¼å¼ï¼Œå†æŒ‰ç…§ç¬¬ä¸€ç§æ–¹å¼è¿›è¡Œå¯¼å…¥ã€‚
æˆ‘ä»¬æ•´ä¸ªè½¬æ¢çš„è¿‡ç¨‹åˆ†ä¸ºä»¥ä¸‹å‡ æ­¥ï¼š
1. ä»huggingfaceä¸Šä¸‹è½½model
2. ä½¿ç”¨llama.cppæ¥è¿›è¡Œè½¬åŒ–
3. ä½¿ç”¨llama.cppæ¥è¿›è¡Œæ¨¡å‹é‡åŒ–
4. è¿è¡Œå¹¶ä¸Šä¼ æ¨¡å‹

### 3.1 ä»HuggingFaceä¸‹è½½Model
æœ€ç›´è§‰çš„ä¸‹è½½æ–¹å¼æ˜¯é€šè¿‡git cloneæˆ–è€…é“¾æ¥æ¥ä¸‹è½½ï¼Œä½†æ˜¯å› ä¸ºllmæ¯éƒ¨åˆ†éƒ½æŒ‰GBè®¡ç®—ï¼Œé¿å…å‡ºç°`OOM Error(Out of memory)`ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨Pythonå†™ä¸€ä¸ªç®€å•çš„download.py
é¦–å…ˆåº”è¯¥å»hfæ‹¿åˆ°ç”¨æˆ·ä¸ªäººçš„`ACCESS_TOKEN`ï¼Œæ‰“å¼€ huggingface ã€‚

![alt text](../images/C3-3-1.png)
![alt text](../images/C3-3-2.png)
![alt text](../images/C3-3-3.png)

è‡³æ­¤ï¼Œæˆ‘ä»¬å°±æ‹¿åˆ°äº†ä¸€ä¸ª`ACCESS_TOKEN`ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬ä½¿ç”¨`huggingface_hub`çš„`snapshot_download`ä¸‹è½½æ¨¡å‹ï¼Œæˆ‘ä»¬è¿™é‡Œé€‰æ‹©çš„è¿˜æ˜¯Qwen-0.5b

```jupyter
!pip install huggingface_hub
```

```python
from huggingface_hub import snapshot_download

model_id = "Qwen/Qwen1.5-0.5B" # hugginFace's model name
snapshot_download(
    repo_id=model_id, 
    local_dir="Qwen-0.5b",
    local_dir_use_symlinks=False,
    revision="main",
    use_auth_token="<YOUR_ACCESS_TOKEN>")
```
æˆ‘ä»¬cdåˆ°ç¬¬ä¸‰éƒ¨åˆ†çš„ç›®å½•ä¸‹æ‰§è¡Œï¼Œä¸‹è½½æ—¶é—´å¯èƒ½ä¼šå¾ˆé•¿ï¼Œè€å¿ƒç­‰å¾…ã€‚

### 3.2 ä½¿ç”¨llama.cppè¿›è¡Œè½¬æ¢
llama.cppæ˜¯GGMLä¸»è¦ä½œè€…åŸºäºæœ€æ—©çš„llamaçš„c/c++ç‰ˆæœ¬å¼€å‘çš„ï¼Œç›®çš„å°±æ˜¯å¸Œæœ›ç”¨CPUæ¥æ¨ç†å„ç§LLMï¼Œåœ¨ç¤¾ç¾¤çš„ä¸æ–­åŠªåŠ›ä¸‹ç°åœ¨å·²ç»æ”¯æŒå¤§å¤šæ•°ä¸»æµæ¨¡å‹ï¼Œç”šè‡³åŒ…æ‹¬å¤šæ¨¡æ€æ¨¡å‹ã€‚

é¦–å…ˆæˆ‘ä»¬å…‹éš†llama.cppåº“åˆ°æœ¬åœ°ï¼Œä¸ä¸‹è½½çš„æ¨¡å‹æ”¾åœ¨åŒä¸€ç›®å½•ä¸‹ï¼š
```jupyter
git clone https://github.com/ggerganov/llama.cpp.git
```
ç”±äºä½¿ç”¨llama.cppè½¬æ¢æ¨¡å‹çš„æµç¨‹åŸºäºpythonå¼€å‘ï¼Œéœ€è¦å®‰è£…ç›¸å…³çš„åº“ï¼Œæ¨èä½¿ç”¨condaæˆ–venvæ–°å»ºä¸€ä¸ªç¯å¢ƒ
```jupyter
cd llama.cpp
pip install -r requirements.txt
python convert_hf_to_gguf.py -h
```
å¦‚æœæ˜¾ç¤ºä»¥ä¸‹å†…å®¹ï¼Œè¯´æ˜è½¬æ¢ç¨‹åºå·²ç»å‡†å¤‡å¥½äº†ã€‚
![alt text](../images/C3-3-4.png)
æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æŠŠåˆšåˆšä»HuggingFaceä¸‹è½½çš„æ¨¡å‹è½¬æ¢ä¸ºGGUFæ ¼å¼ï¼Œå…·ä½“ä½¿ç”¨ä»¥ä¸‹è„šæœ¬ï¼š
```bash
python convert_hf_to_gguf.py ../Qwen-0.5b --outfile Qwen_instruct_0.5b.gguf --outtype f16
```
![alt text](../images/C3-3-5.png)

å¯ä»¥çœ‹åˆ°llama.cppç›®å½•ä¸‹å¤šäº†ä¸€ä¸ªQwen_instruct_0.5b.ggufæ–‡ä»¶ï¼Œè¿™ä¸ªè¿‡ç¨‹åªéœ€è¦å‡ ç§’é’Ÿã€‚

ä¸ºäº†èŠ‚çœæ¨ç†æ—¶çš„å¼€é”€ï¼Œæˆ‘ä»¬å°†æ¨¡å‹é‡åŒ–ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å¼€å§‹é‡åŒ–å®æ“ã€‚


### 3.3 ä½¿ç”¨llama.cppè¿›è¡Œæ¨¡å‹é‡åŒ–
æ¨¡å‹é‡åŒ–æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œå°†é«˜ç²¾åº¦çš„æµ®ç‚¹æ•°æ¨¡å‹è½¬æ¢ä¸ºä½ç²¾åº¦æ¨¡å‹ï¼Œæ¨¡å‹é‡åŒ–çš„ä¸»è¦ç›®çš„æ˜¯å‡å°‘æ¨¡å‹çš„å¤§å°å’Œè®¡ç®—æˆæœ¬ï¼Œå°½å¯èƒ½ä¿æŒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå…¶ç›®æ ‡æ˜¯ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šè¿è¡Œï¼Œä¾‹å¦‚CPUæˆ–è€…ç§»åŠ¨è®¾å¤‡ã€‚

åŒæ ·çš„ï¼Œæˆ‘ä»¬å…ˆåˆ›å»ºModelfileæ–‡ä»¶ï¼Œå†ä½¿ç”¨ollama createå‘½ä»¤æ¥ä»ggufæ–‡ä»¶ä¸­åˆ›å»ºæˆ‘ä»¬çš„æ¨¡å‹ï¼Œä¸è¿‡ä¸ç¬¬ä¸€æ­¥ç¨æœ‰ä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬æ·»åŠ äº†é‡åŒ–é€»è¾‘ï¼Œåªéœ€è¦åœ¨æ‰§è¡Œollama createæ˜¯æ·»åŠ ä¸€ä¸ªå‚æ•°å³å¯ã€‚

é¦–å…ˆæŠŠä¸Šä¸€æ­¥æ‹¿åˆ°çš„Qwen_instruct_0.5b.ggufç§»åŠ¨è‡³ç¬¬ä¸‰éƒ¨åˆ†çš„æ ¹ç›®å½•ä¸‹ï¼Œå†åˆ›å»ºModelfileæ–‡ä»¶ç¼–å†™ä»¥ä¸‹å†…å®¹.

```python
FROM ./Qwen_instruct_0.5b.gguf
```

ç»ˆç«¯è¿è¡Œåˆ›å»ºå’Œé‡åŒ–è„šæœ¬ã€‚

```python
# ç¬¬ä¸‰éƒ¨åˆ†æ ¹ç›®å½•ä¸‹
ollama create -q Q4_K_M mymodel3 -f ./Modelfile
```
![alt text](../images/C3-3-6.png)

åˆ°æ­¤ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å°±é‡åŒ–å¹¶åˆ›å»ºå®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°±å¯ä»¥è¿è¡Œæ¨¡å‹äº†ã€‚

### 3.4 è¿è¡Œå¹¶ä¸Šä¼ æ¨¡å‹

ä½¿ç”¨ggufè¿è¡Œæ¨¡å‹çš„æ­¥éª¤è¯¦è§ç¬¬ä¸€éƒ¨åˆ†ï¼Œè¿™é‡Œä¸å†èµ˜è¿°ã€‚

æœæœ¬åœ°ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶å¤ªå ç”¨ç©ºé—´ï¼Œå¯ä»¥ä¸Šä¼ ggufæ¨¡å‹åˆ°huggingfaceçš„è‡ªå·±çš„repoä¸­ï¼ŒåŒæ­¥éª¤ä¸€çš„æ€æƒ³ï¼Œæˆ‘ä»¬å¯ä»¥å†™ä¸€ä¸ªuploadçš„é€»è¾‘ã€‚

> tipsï¼šå¦‚æœæƒ³å®Œæˆä¸Šä¼ ï¼Œä½ çš„HF_ACCESS_TOKENæƒé™å¿…é¡»è¦ä¸ºwriteï¼Œå¹¶ä¸”è¦ä¿®æ”¹ä½ çš„model_idï¼Œyour_hf_nameæŒ‡çš„æ˜¯ä½ huggingfaceè´¦å·åç§°

```python
from huggingface_hub import HfApi
import os

api = HfApi()
HF_ACCESS_TOKEN = "<YOUR_HF_WRITE_ACCESS_TOKEN>"
#TODO è¿™é‡Œéœ€è¦è®¾ç½®ä½ çš„model_id
#ä¾‹å¦‚ model_id = "little1d/QWEN-0.5b"
model_id = "your_hf_name/QWEN-0.5b"


api.create_repo(
    model_id,
    exist_ok=True,
    repo_type="model", # ä¸Šå‚³æ ¼å¼ç‚ºæ¨¡å‹
    use_auth_token=HF_ACCESS_TOKEN,
)
# upload the model to the hub
# upload model name includes the Bailong-instruct-7B in same folder
for file in os.listdir():
    if file.endswith(".gguf"):
        model_name = file.lower()
        api.upload_file(
            repo_id=model_id,
            path_in_repo=model_name,
            path_or_fileobj=f"{os.getcwd()}/{file}",
            repo_type="model", # ä¸Šå‚³æ ¼å¼ç‚ºæ¨¡å‹
            use_auth_token=HF_ACCESS_TOKE)

```

![alt text](../images/C3-3-7.png)

ä¸Šä¼ å®Œæˆåå°±å¯ä»¥åœ¨è‡ªå·±çš„hfä»“åº“ä¸­çœ‹åˆ°å•¦ï¼

## å››ã€è‡ªå®šä¹‰Prompt

Ollamaæ”¯æŒè‡ªå®šä¹‰Promptï¼Œå¯ä»¥è®©æ¨¡å‹ç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·éœ€æ±‚çš„æ–‡æœ¬ã€‚

è‡ªå®šä¹‰Promptçš„æ­¥éª¤å¦‚ä¸‹ï¼š
1. æ ¹ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªModelfileæ–‡ä»¶

    ```python
    FROM llama3.1
    # sets the temperature to 1 [higher is more creative, lower is more coherent]
    PARAMETER temperature 1
    # sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token
    PARAMETER num_ctx 4096

    # sets a custom system message to specify the behavior of the chat assistant
    SYSTEM You are Mario from super mario bros, acting as an assistant.
    ```
2. åˆ›å»ºæ¨¡å‹

 ```python
 ollama create mymodel -f ./Modelfile
 ```
 ![alt text](../images/C3-4-1.png)

 åˆ›å»ºæ¨¡å‹çš„æ—¶é—´å¯èƒ½ç¨å¾®ä¼šä¹…ä¸€ç‚¹ï¼Œå’Œpullä¸€ä¸ªæ¨¡å‹çš„æ—¶é—´å·®ä¸å¤šï¼Œè¯·è€å¿ƒç­‰å¾…

 ![alt text](../images/C3-4-2.png)


 å†æ¬¡è¿è¡Œ`ollama list`æŸ¥çœ‹å·²æœ‰çš„æ¨¡å‹ï¼Œå¯ä»¥çœ‹åˆ°mymodelå·²ç»æ­£ç¡®åˆ›å»ºäº†


 ![alt text](../images/C3-4-3.png)


3. è¿è¡Œæ¨¡å‹

 ```python
 ollama run mymodel
 ```
 ![alt text](../images/C3-4-4.png)
 å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„å°ç¾Šé©¼ğŸ¦™å·²ç»å˜æˆäº†Marioï¼è‡ªå®šä¹‰PromptæˆåŠŸï¼ğŸ˜˜ğŸ˜˜



## å‚è€ƒé“¾æ¥
- https://www.linkedin.com/pulse/llama-3-safetensors-vs-gguf-talles-carvalho-jjcqf
- https://www.sysgeek.cn/ollama-on-windows
- https://ollama.com/blog/openai-compatibility